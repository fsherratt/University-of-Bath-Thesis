\chapter{Model Personalisation}
\label{chp:personalisation}

The previous chapter investigated classification accuracy for a general or subject agnostic \acrshort{lstm} based \acrshort{lmr} model. Due to the variability between individuals classification accuracies of greater than 80\% could not be achieved for unseen novel subjects. Instead a form of personalisation is necessary to adapt the model for novel subjects. Within this chapter methods for achieving this will be explored. Before attempting to produce a model for amputees methods will be developed and tested on non-amputees.

The collection of labelled data for an individual is burdensome therefore any system that can be used to reduce the labelling requirements is advantageous. Transfer learning approaches allow large training sets pooled from many individuals to be leveraged to reduce target data requirements on the assumption they hold relevant information.\cite{Fallahzadeh2017, Schneider2021} 

From this point forward the following naming convention will be used; the subject of personalisation will be referred to as the target and all other subjects will be referred to as source.

Within this chapter we will investigate whether a large population of source data can be used to improve the performance and efficiency of producing a personalised \acrshort{lstm} \acrshort{lmr} classifier for a target individual.

The contributions of this Chapter are as follows:
\begin{itemize}
    \item A method for evaluating personalisation \acrshort{lmr} models from a set of real world continuous gait data
    \item Demonstration of the impact on classification performance of increased target training data
\end{itemize}

The Chapter is presented as follows. First, in Section \ref{sec:personalisation-related-works}, related literature is presented. This is followed by the methods and materials used in the study in Section \ref{sec:personalistaion-methods}. The results of a baseline model trained from only target data are presented in Section \ref{sec:personalisation-baseline-model-results}, followed by the results and analysis for penalisation techniques in Section \ref{sec:model-personalisation-results}. Finally discussion and conclusions are presented in Sections \ref{sec:personalisation-discussion} and \ref{sec:personalisation-conclusions} respectively.

%------------------------------------------
\section{Related Works} % Does this need more detail to back up the research gaps identified
\label{sec:personalisation-related-works}
\acrshort{ml} classifiers are constructed with the assumption that the probabilistic distribution between the source and target domain are equal\cite{Farahani2020}. In reality this is never the case and as such methods to account for differences between domains have been developed. Where this is adaptation between humans this is typically termed personalisation.

Personalising of \acrshort{ml} models is a common issue and has been addressed in many different ways across different areas of research\cite{Mairittha2021, Tomanek2021}. Schneider et al divides personalisation methods into two groups, shaping and data grouping\cite{Schneider2021}. In shaping the behaviour of a network is biased or shaped towards an individual, in data grouping the target data set is enlarged by adding data from similar individuals to it. Both of these techniques take advantage of data from others to reduce labelled data requirements for the target subject\cite{Shor2020}. The following survey of literature will be divided by these two categories. 

Shaping the behaviour of a network can occur at different times during training. Two are common, the beginning, early, or the end, late. In early shaping the model is first trained with target data followed by a larger set of source data. The opposite is done in late shaping, where a general model formed from a large source training set is fine-tuned with target data. By updating a trained model using data from a different distribution, the knowledge from an existing model can be transferred. This method is common and normally referred to as transfer learning.\cite{Schneider2021}

Transfer learning is the ability to extend what has been learnt in one context to another nonidentical but similar context\cite{Fallahzadeh2017}. The change in context can be either the task, the domain or both. Transfer learning is appealing, since it is often faster, as a model does not need to be trained from scratch for each target. 

Transfer learning is generally achieved in two phases. First a generic global model is trained from a set of source data. Then it is adapted to the target by additional training using only the targets data. The influence of the target is controllable by both the number of iterations and number of layers trained.\cite{Schneider2021, Mireshghallah2021}

A subset of transfer learning is domain adaptation where the domain changes but the task remains the same\cite{Goodfellow2015}. Domain adaptation techniques often focus on learning and apply a mapping between the source and target input data rather than fine-tuning an existing model.

%-----------------------------
% Shaping/Transfer learning
Yoon et al presented a transfer learning scheme for personalisation of a \acrshort{lstm} based language model for generating stylised sentence completion. Their work focuses on techniques that allow transfer learning using only a small amount of target data and limited computing resource. To achieve this two schemes are investigated - inserting and training a surplus layer between the output and last LSTM layer; and freezing the first n-layer and fine-tuning just the subsequent layers. Both methods reduce the training requirements when compared to fine-tuning the entire base model while achieving similar performance.\cite{Yoon2017}

Fu et al developed a domain adaptation method for unlabelled target data denoted Joint Probability Domain Adaptation with Improved Pseudo Labels (IPL-JPDA). The method produces a transformation matrix to adapt the input data of the target to the source domain removing the need to adjust the model itself. The study collects labelled data for a set of ten subjects in a controlled environment. This data is then split into target and source data sets with the performance tested using a cross validation method. Personalisation is undertaken using the IPL-JPDA method and tested against a subset of the data windows for each activity. Their method achieves an accuracy of $93.2\%$ accuracy, an increase of around $2\%$ over the baseline.\cite{Fu2021}


%-----------------------------
% Data grouping - Training from similar users
The other category of personalisation is data grouping. In data grouping the target data set is enlarged by supplementing it with data from existing source data. Each individual will differ from each other, but it should be expected that the population as a whole or a subset of it are similar\cite{Schneider2021, Nguyen2021}. Identifying and combining similar individuals is the main area of concentration for this field.

Ferrari et al investigated data grouping personalisation methods that weight the influence of training data based on similarity to the target subject. Similarity was evaluated through physical traits (age, weight and height) and on comparison of the input feature vector. Three publicly available \acrshort{adl} data sets were used to test performance, niMiB-SHAR\cite{Micucci2017}, Mobiact\cite{Vavoulas2016} and Motion Sense\cite{Katevas2014}. All data was collected in controlled conditions. An Adaboost classifier was trained for each target subject using the weighted training data. The experiment was repeated with and without target data included in the training set. Excluding the target saw only a small improvement in performance, compared to without similarity biasing. Including the target in the training data increased classification accuracy by $>10\%$, on average achieving $87.39\%$. This suggests weighting the training data set towards the target subject has a larger influence on performance than similarity.

Nguyen et al presented another data grouping technique using a DeepConvLSTM architecture. The model used learnt features, so determining the similarity of the feature vector was not possible. Instead the output of the last LSTM layer was used as a pseudo for the feature vector. A \acrfull{fid} algorithm was used to score the similarity of subjects. The score was then used to group source subjects, by selecting the closest $n$ neighbours and also clustering subjects into communities. It was noted this correlated closely with the physical characteristics. The groups were then used to both train a new model from scratch and fine-tune a general model. Fine-tuning a global model proved more effective. This method improved the global model performance by $3.5\%$ to $84.2\%$. The experiments were performed on four public data sets; OPPORTUNITY\cite{Roggen2009}, Daphnet Gait\cite{Sigcha2020}, Wetlab\cite{Scholl2015} and Mobiact\cite{Vavoulas2016} data sets, all of which were collected in closed controlled environments.\cite{Nguyen2021}


%-----------------------------
% Combination - retrain general model based on similar users
Several authors attempted to combine both transfer learning and data grouping techniques. These methods used data grouping techniques to produce a base model which was subsequently fine-tuned using data from the target.

Wang et al presents a source selection and transfer learning approach for a \acrshort{cnn}-\acrshort{lstm} architecture for unsupervised transfer learning. First source subjects were selected based on a closeness score. This score was a combination of a cosine similarity function and a hand selected value based on physical similarity between sensor locations. Using the selected source subjects a \acrshort{ml} model was trained. Fine-tuning of the model was achieved by inserting and training an adaption layer between the last two dense layers. The investigation was performed using the OPPORTUNITY\cite{Roggen2009}, PAMAP2\cite{Reiss2012} and UCI DSADS\cite{Altun2010} data sets which again were all collected in controlled conditions.\cite{Wang2018a}

Cruciani et al presents work on personalising an activity recognition model built from the subset of a general population. The subset of subjects was selected by comparing the similarity of manually selected features. Those with the closest matching traits were used to generate the base model. Further training was then performed using a small amount of target  data. This approach achieved a ~5\% improvement in performance when compared to selecting a source subset at random\cite{Cruciani2020}. The experiment was performed on the \acrshort{adl} Extrasensory data set published by Vaizman et al\cite{Vaizman2017}, this data set was collected using a smartphone in uncontrolled conditions with limited guidance given on how to collect or label the data.

From the literature surveyed the following gaps were identified:
\begin{itemize}
\item No methods for effectively testing the general performance of a transfer learning system from continuous real world environments
\item No comparison between the performance that could be achieved by just training a model with the target subject's data
\item There has been limited work on understanding how performance improves with increasing amounts of target data
\item None have investigated the use of IMU ankle data
\end{itemize}


%------------------------------------------
\section{Methods and Materials}
\label{sec:personalistaion-methods}
%Introduction to section
Within this section, the methods and materials required to address the research question will be detailed. The section is structured as follows: first, details of an expanded data set of labelled real world HAR data are provided; then, new methods for dividing this data into representative data sets are developed; finally, \acrshort{ml} personalisation methods are presented.

% Data collection
\subsection{Gait Data}
For these experiments a HAR data set, which contains both a large population and a large quantity of data for a small subset, is required. Data for a large number of subjects has previously been collected. Therefore only additional data for the subset of target subjects is required. These will be Subjects 01, 03 and 09. The additional data was collected in the same manner as before, described in Section \ref{sec:methods-data-collection}.

Only data from the shank mounted accelerometer and gyroscope will be used. As from previous work, in Chapter \ref{chp:lstm-general}, minimal performance improvement was seen for the additional sensors. 

%------------------
% Data augmentation (Combining left and right ankle data)
To reduce the data required for the target subject, data for both the left and right ankle was combined. The transformation in Equation \ref{eqn:left-right-transformation} was used to rotate and reflect the left ankle to match the right ankle. In Equation \ref{eqn:left-right-transformation} $V$ is the original data and $V_t$ is the transformed data. 

Figure \ref{fig:personalistaion_target_subjects_gyro_trends} shows the mean signals from the shank mounted gyroscope in the saggital plane for each of the target subjects. Note data is not normalised. Only stair descent for subject 3 shows any obvious differences between left and right ankles. Therefore it is reasonable to combined ankle data in this way.

\begin{equation}
    V_t = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & -1 & 0 \\
    0 & 0 & -1
    \end{bmatrix} V
\label{eqn:left-right-transformation}
\end{equation}

\begin{figure}[p]
    \begin{tabular}{lccc}
        & \textbf{Subject 1} & \textbf{Subject 3} & \textbf{Subject 9} \vspace{0.2cm}\\
        \rotatebox{90}{\enspace\qquad \textbf{Walking}} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_walking.pdf}\end{subfigure} & \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_walking.pdf}\end{subfigure} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_walking.pdf}\end{subfigure} \\
        \rotatebox{90}{~\quad \textbf{\glsentrylong{ra}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_up.pdf} \\
        \rotatebox{90}{\quad \textbf{\glsentrylong{rd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_down.pdf} \\
        \rotatebox{90}{~\quad \textbf{\glsentrylong{sa}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_up.pdf} \\
        \rotatebox{90}{\quad \textbf{\glsentrylong{sd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_down.pdf} \\
    \end{tabular}
    \centering
    \caption[Angular velocity of the shank in the Saggital Plane during different activities for the three target subject]{Angular velocity of the shank in the Saggital Plane during different activities for the three target subject. The solid line shows the mean angular velocity for all steps recorded for each activity. The filled area represents the standard deviation. 0\% gait cycle is taken as peak swing for simplicity of calculation. The red, green and yellow lines are for the left ankles of Subjects 01, 03 and 09 respectively. The blue, purple and grey lines show the right ankles of Subjects 01, 03 and 09 respectively.}
    \label{fig:personalistaion_target_subjects_gyro_trends}
\end{figure}

\subsection{Data Division}
The HAR data set is made of a series of continuous recordings which may cover multiple different activities and environments. Developing an effective method for dividing this data will be critical to demonstrating the effectiveness of personalisation.

It is highly likely to suffer from poor distribution of classes since activities such as walking are far more prevalent than climbing stairs. As \acrshort{ml} methods perform best using balanced data sets, this must be corrected. Additionally, in order for the test set to represent a novel environment, the training data sets should ideally not include any data from the same environment. Therefore each unique episode should only be used once across the training and test data sets. Finally the data division method should allow for multiple repeatable unique sets to be constructed to allow for cross-validation of performance. Achieving all these requirements means that the recordings cannot simply be divided by time.

The proposed approach is to divide the continuous data of each subject into episodes, each containing one continuous period of activity. Episodes can then be combined to form the three independent data sets. Each episode is only used once, any excess episodes are discarded. To balance the number of classes, excess windows are discarded randomly from all episodes. To produce cross-validation sets the starting order of the episodes can be shuffled. Figure \ref{fig:methods-per-episode-data-division} illustrates the process of forming the three data sets.

 \begin{figure}[hbt]
     \centering
     \includegraphics[width=0.9\textwidth]{content/3-Methods/Episode_Division.pdf}
     \caption[Per-episode data division]{Per-episode data division. Step 1 -- Labelled data files for a participant are loaded. Step 2 -- Episodes of the same activity are grouped together. Step 3 -- Training, Validation and Test sets are formed by stacking episodes until the required window quantity reached.}
     \label{fig:methods-per-episode-data-division}
 \end{figure}
 
%BOOKMARK (SM) - PROOF READ UP TO HERE
For all experiments the test sets will contain 5000 windows of target data. Training/validation set will vary in length. For conciseness the number of training windows will be presented as the sum of both training and validation windows. These will always be in the ratio 70:30. 

Each experiment will be repeated multiple time with episodes randomly shuffled between each to improve statistical certainty. The shuffling will be repeatable and test data will always be drawn first to ensure a consistent test set.

The time in seconds can be calculated using Equation \ref{eqn:episode_set_length_seconds}, where $T$ is total set length in seconds, $f_s$ is the sampling frequency, $s_k$ is the window skip value, $n_w$ is the number of windows, $l_w$ is the window size, and $n_e$ is the number of episodes included in the set.

\begin{equation}
    T = \frac{1}{f_s}(s_k n_w + l_w n_e)
    \label{eqn:episode_set_length_seconds}
\end{equation}

Calculating the actual quantity of data used in seconds is non-trivial as some windows may be dropped during class balancing. Assuming no windows are dropped and only one episode is used 5000 windows uses a minimum of 151 seconds for each class. $l_w$ set to 128, $f_s$ set to 100Hz, and $s_k$ equal to three.

%--------------------------------
% Machine learning methods
\subsection{\glsentrylong{ml} Methods}
Two personalisation methods will be evaluated -- data supplementation and transfer learning. These will be compared against two baselines, a model trained using only target training data, and a general subject agnostic model.

The data supplementation technique will mix source and target data to produce a larger training set. This will then be used to train a new classifier from scratch. The additional data will be selected randomly without attempting to match similar subjects. The amount of both source and target data will be varied to investigate the impact of both.

The transfer learning approach will fine-tuned a set of base general models using data from a target subject. The base models will be generated by training a model from scratch using the full source data excluding the three target subjects. Five base model will be produced, by randomly shuffling the training and validation.

Personalisation will occur by additional training using only target data to attempt to shape the model. The amount of target training data used will be varied to asses the impact of this of classification performance. Three different training configurations will be tested, each configuration will vary by which layers are trained. For the first configuration all layers will be fine-tuned, the second and third method will train only the \acrshort{lstm} and dense layers respectively.

The same \acrshort{lstm} architecture will be used throughout all experiments shown in Figure \ref{fig:ch5_illustration_of_base_LSTM_model}. This is the same architecture as used in Chapter \ref{chp:lstm-general}. The first has an \acrshort{lstm} layer than takes a 128 x 6 input of raw \acrshort{imu} data. The \acrshort{lstm} layer can be a varying number of units wide but will always be 128 units long. The full output of this layer is then passed to a dense late fusion layer before been passed through a \acrshort{relu} classifier. All training hyper-parameters were tune empirically.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{content/5-Personalisation/ch5_lstm_architecture.pdf}
    \caption[Illustration of \glsentryshort{lstm} machine learning model architecture]{Illustration of \acrshort{lstm} machine learning model architecture}
    \label{fig:ch5_illustration_of_base_LSTM_model}
\end{figure}

All training will be undertake using the same methods as described in Chapter \ref{chp:lstm-general}. The full set of windows will be passed thought the training systems in mini-batches of 100 windows. After every epoch the validation set will be used to evaluate the performance of the model. When the categorical loss of the validation set stagnates for more than 3 epochs training will be stopped.

Model performance will be assessed primarily by the classification accuracy using the unseen test data set. Additionally measures include the number of epochs, training time and quantity of training data required. These will aim in determining the computationally/data efficient. By using comparison against the baselines it will be possible to determine if these methods are of benefit.


%-------------------------------------------------------------------------------
\section{Baseline Model Performance}
\label{sec:personalisation-baseline-model-results}
To determine if personalisation has resulted in an improvement a performance baseline is required. Two baselines will be generated for each target subject. These will be the accuracy of a general model for the target, and the accuracy of a model trained using only target data. If performance of the personalisation methods do not exceed the baselines there is no benefit in them. The performance of both baselines is presented within this section.

% Performance of the trained general models with no fine-tuning
For the first baseline the classification accuracy of the general models when presented with the test data sets was evaluated. The average accuracy of the five models was $76.5\%\pm3.1$ for Subject 1, $81.5\%\pm4.3$ for Subject 3 and $65.8\%\pm2.7$ for Subject 9.

To determine a baseline for models trained with only target training data LSTM models of different shapes were trained using increasing amounts of target data. Figure \ref{fig:ch5_bespoke_mode_classification} shows the classification performance for each of subject using different quantities of target data windows for 6, 16, 32 and 64 unit \acrshort{lstm} networks. The full data tables are available in Appendix \ref{chp:tables-of-results} Section \ref{sec:appendix-a-model-performance-bespoke}.

% Confusion matrix
\begin{table}[p]
    \centering
    \caption[Test data confusion matrix for a 32 unit LSTM model trained with 15000 target data window]{Test data confusion matrix for a 32 unit LSTM model trained with 15000 target data window. Columns represent the prediction labels and the rows represent the real labels. Each value represent the percentage of total predictions of that class. (\acrfull{ra}, \acrfull{rd}, \acrfull{sa}, \acrfull{sd})}
    \label{tab:ch5-bespoke-mode-confusion-matrix_subject_09}
    \begin{subtable}{\textwidth}
    \caption{Subject 1}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 71.3 & 20.6 & 18.1 & 0.4 & 1.9 & 2.7 \\
         & \glsentryshort{ra} & 25.9 & 72.4 & 0.0 & 0.0 & 0.0 & 0.0 \\
         & \glsentryshort{rd} & 1.2 & 0.0 & 80.0 & 0.2 & 6.2 & 0.0 \\
         & \glsentryshort{sa} & 0.9 & 5.6 & 0.2 & 92.9 & 5.2 & 0.2 \\
         & \glsentryshort{sd} & 0.8 & 1.2 & 1.7 & 4.5 & 86.7 & 1.7 \\
         & STOP               & 0.1 & 0.1 & 0.0 & 2.0 & 0.1 & 95.4 \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{Subject 3}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 50.3 & 3.3 & 4.2 & 0.2 & 4.1 & 3.1 \\
         & \glsentryshort{ra} & 32.3 & 52.1 & 0.3 & 0.0 & 0.0 & 0.0 \\
         & \glsentryshort{rd} & 16.5 & 43.5 & 92.1 & 0.1 & 0.5 & 0.0 \\
         & \glsentryshort{sa} & 0.5 & 0.8 & 0.2 & 99.2 & 1.5 & 1.0 \\
         & \glsentryshort{sd} & 0.2 & 0.2 & 3.1 & 0.5 & 94.0 & 0.1 \\
         & STOP               & 0.1 & 0.0 & 0.0 & 0.0 & 0.1 & 95.7 \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{Subject 9}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 90.4 & 7.6 & 6.3 & 2.0 & 2.7 & 0.0 \\
         & \glsentryshort{ra} & 2.2 & 82.5 & 0.1 & 1.4 & 0.1 & 0.0 \\
         & \glsentryshort{rd} & 5.6 & 7.4 & 85.1 & 2.1 & 8.8 & 0.0 \\
         & \glsentryshort{sa} & 1.0 & 1.7 & 0.6 & 92.8 & 4.7 & 0.1 \\
         & \glsentryshort{sd} & 0.8 & 0.6 & 8.0 & 1.6 & 83.4 & 0.7 \\
         & STOP               & 0.1 & 0.2 & 0.0 & 0.2 & 0.4 & 99.2 \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
\end{table}

% Classification performance
\begin{figure}[p]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_1.pdf}
        \caption{Subject 1}
        \label{fig:ch5_6_unit_bespoke_model}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_3.pdf}
        \caption{Subject 3}
        \label{fig:ch5_16_unit_bespoke_model}
    \end{subfigure}
    \caption[Classification performance of different size \glsentryshort{lstm} networks trained with varying amount of target subject data]{Classification performance of different size \acrshort{lstm} networks trained with varying amount of target subject data. The solid lines represent the mean of all models trained, the filled area represents the standard deviation $(n=10)$. Each line show the classification performance for a different number of \acrshort{lstm} units. The red dot is 6 units, blue plus 16, purple cross 32 and yellow asterisk 64.}
    \label{fig:ch5_bespoke_mode_classification}
\end{figure}
\begin{figure}[t]\ContinuedFloat
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_9.pdf}
        \caption{Subject 9}
        \label{fig:ch5_32_unit_bespoke_model}
    \end{subfigure}
    \caption[]{Classification performance of different size \acrshort{lstm} networks trained with varying amount of target subject data (Cont.).}
\end{figure}

%BOOKMARK - (FS) REWRITTEN UP TO HERE
The maximum performance achieved was 84.4\% for Subject 1, 88.5\% for Subject 3, and 82.6\% for Subject 9. This was achieved at 15000 windows for Subjects 1 and 3 but 9000 samples for Subject 9. It's not clear why performance decreased after this point. 

The fastest rate of performance improvement was seen early on, from 100 to 1500 data windows. Beyond this there was a more gradual increase in performance. It appears that performance would have continued to improve the maximum number of windows tested. Indicating further data would still improve performance. 

Standard deviation reduced with increasing quantities of data windows indicating more consistent performance across all test sets as as the model was exposed to more data.

Increasing the number of units in general improved classification performance. This levels off at 32 units. Only the 6 unit model appears to have insufficient learning capacity. Increasing the number of units also reduced the number of epochs required to train the models. Therefore 32 units is likely a good candidate for future models.

The reduction in performance at 3000 samples for Subject 3 is likely due to model exposure to a new environment of data. Performance recovers with increasing amounts of data. Subject 9 also experiences similar drops in performance.

%Why is the model not achieving better performance?
An assessment of where classification errors are occurring can be made by looking at the confusion matrices. Table \ref{tab:ch5-bespoke-mode-confusion-matrix_subject_09} shows confusion matrices for the three targets classifiers created using 15000 training windows. Performance is averaged across all test/training sets. The value in each cell is the percentage of total predictions of each class.

From the confusion matrices it can be seen that stop achieves the highest accuracy, greater than $95\%$ for all target subjects. This is the most distinct class so this is not surprising. Some of the errors observed may be due to mislabelling of classes

Stairs were also identified fairly accurately with \acrlong{sa} achieving greater than $92\%$ accuracy and \acrlong{sd} greater than $83\%$.

All three subjects classifiers struggled to separate walking, \acrlong{ra} and \acrlong{rd} from each other. With as low as $50\%$ accuracy for subject 3.

This performance leaves plenty of room for improvement. Especially given requiring over seven minutes of each class to achieve greater than $80\%$ accuracy is impractical.


%-------------------------------------------------------------------------------
\section{Model Personalisation}
\label{sec:model-personalisation-results}
The results and analysis for both personalisation methods described in Section \ref{sec:personalistaion-methods} are presented below.

%-------------------------------------------------------------------------------
\subsection{Data Supplementation}
A series of \acrshort{lstm} models were trained using different quantities of source and target windows. The experiment aimed to establish if the addition of source data improves the classification performance.

Tables \ref{tab:ch5-mixed-target-and-source-data-subject-01}, \ref{tab:ch5-mixed-target-and-source-data-subject-03} and \ref{tab:ch5-mixed-target-and-source-data-subject-09} present the results data supplementation experiments. Each cell contains the mean classification accuracy for target training and standard deviation. Columns represent different quantities source training windows. Table rows represent different quantities of target training windows. The highest classification accuracy for each quantity of target training windows has been highlighted in bold.

% Fully trained model
% Present performance on this model - Categorical accuracy of training data, learning rate (epochs vs categorical accuracy)
\input{content/5-Personalisation/mixed-target-source-data-tables}

No obvious trends

Except for low values of source data with high values of target data - adding source data improves classification performance. 

More source windows than target windows always results in a increase in classification accuracy

Classification performance increases with increasing source data before falling off. It's not obvious where the point of degradation occurs. There is a band where performance is highest

Long training times - large number of epochs

%-------------------------------------------------------------------------------
\subsection{Transfer Learning}
Classifiers were training for each of the three target subjects by fine-tuning the general models produced earlier. Figure \ref{fig:ch5_pretrained_model} shows the classification performance for the three methods of fine-tuning the general models with increasing amounts of target training windows. The three methods are fine-tuning the whole network, only fine-tuning the dense layer and only fine-tuning the \acrshort{lstm} layer.

\begin{figure}[p]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_pre_trained_model_accuracy.pdf}
        \caption{Fine-tuning all layers}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_frozen_lstm_layer_accuracy.pdf}
        \caption{Fine-tuning only the dense layer}
    \end{subfigure}
    \caption[Results of fine-tuning a generic 32 unit \glsentryshort{lstm} model using increasing amounts of target data]{Results of fine-tuning a generic 32 unit \acrshort{lstm} model using increasing amounts of target data. The solid line represents the mean classification performance for each amount of training windows. The filled area represents the standard deviation $(n=25)$. Each of the 3 target subjects is represented individually. The red dot line is Subject 1, blue plus is subject 3 and yellow cross is subject 9.}
    \label{fig:ch5_pretrained_model}
\end{figure}
\begin{figure}[t]\ContinuedFloat
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_frozen_dense_layer_accuracy.pdf}
        \caption{Fine-tuning only the \acrshort{lstm} layer}
    \end{subfigure}
    \caption[]{Results of fine-tuning a generic 32 unit \acrshort{lstm} model using increasing amounts of target data (Cont.).}
\end{figure}

Other than 100-200 for subject 3 all values are better than the baseline general model performance. These two values saw a small amount of negative transfer with a reduction in performance.

Very large improvement in performance early on

For subjects 03 and 09 performance drops down to the near the baseline performance around 3000 target samples. This is still an improvement over the general model for Subject 9 but drops below the general model for Subject 3

Freezing LSTM layer improved performance of Subject 1 by >1\%. Near no improvement for Subjects 03, and 09.

Freezing the dense layer improved performance of Subject 3, no improvement for 09 and reduction in performance for 01. There's no obvious reasons for these changes.


%-------------------------------------------------------------------------------
\section{Discussion}
\label{sec:personalisation-discussion}
%What were we trying to achieve and have we achieved it?
Reduce data requirements for a new subject - yep
Reduce training requirements - using transfer learning yep

%Do any of the methods show promise?
All of the methods showed a substantial performance improvements over the general model

All methods improved performance over the baseline model for the same quantity of target training samples

%Any interesting observations?
% Baseline model
More data would have improved things further. Not realistic to expose the model to every possible environment that it will need to operate on.

Interesting how performance degrades at different increasing amounts of data. Suspicion is that this is because of the introduction of new environments of data. Limited control on data collection demonstrates problems with real world data. Encountering a new environment or mislabelling of data can result in degraded performance. Would be interesting to investigate how introduction of new environments affect performance but a new/different data set collected in a controlled manner would be required for this.

This performance was 

% Data grouping
Performance was always better than the baseline model, when more source data than target data was used.

Mixed source and target data achieved better than transfer learning for lower amounts of target data windows. No predictable pattern to better classifications. Cost of substantially higher training epochs with more data so much slower.

With more selective source subjects this would probably have worked better. Additional measures would be needed to evaluate similarity between subjects, these are not needed when using purely deep learning approach.

%Transfer learning
Performance was always better than the baseline.

Improved over the baseline - performance was more predictable although there was still inconsistencies due to the training data sets used. This was similar behaviour to the baseline model.

Not obvious if freezing layers helped likely just noise


% Which was the best model?
Data supplementation worked but it's difficult to predict how much additional data to add so would be difficult to implement in practice

For subjects 01 and 09 transfer learning is a better approach for subject 3 it's less obvious.


%Identify requirements for implementing these methods (data, computation)
Biggest jumps in performance over the baseline and the general model were shown for lower target samples. The 

%What are the limitations of this study/methods?
Data supplementation technique is imprecise - it improves performance but more precision in the selection of supplemental data would probably improve things further 

Reduced transition data from data set, previous work indicated these were a large source in inaccuracy.


%Areas for further work/improvements?
Lots of room for hyper-parameter investigations - only loosely tuned

Data grouping techniques to improve performance of data supplementation
Could combine both methods - transfer learning with data grouping?

Bias the data set towards the target data, using the general population to supplement the limited environments experienced by the subject. Similar to Balanced Batch Learning \cite{Cruciani2020}

%-------------------------------------------------------------------------------
\section{Conclusions}
\label{sec:personalisation-conclusions}
What were the research aims?

Have we met the research aims/outcomes of this work?
Yes we have demonstrated methods for improving the classification performance of \acrshort{lmr} networks using a large source population to improve performance.

What are we going to do next?
Apply methods to amputee data and determine if they are applicable