\chapter{Model Personalisation}
\label{chp:personalisation}
% The first job of the introduction is to tell the reader what your topic is and why it’s interesting or important. This is generally accomplished with a strong opening hook. The hook is a striking opening sentence that clearly conveys the relevance of your topic. Think of an interesting fact or statistic, a strong statement, a question, or a brief anecdote that will get the reader wondering about your topic.
The previous chapters work focused on producing a general \acrshort{ml} model that could provide high classification accuracy for novel unseen subjects. The results of this work suggested that high levels of accuracy were not possible and some form of model personalisation is required.


% Relevant background on the topic - For a paper describing original research, you’ll instead provide an overview of the most relevant research that has already been conducted. This is a sort of miniature literature review—a sketch of the current state of research into your topic, boiled down to a few sentences.
Previously the best performance achieved was approximately 80\% when the general model was presented with a novel subject.
Recap findings and shortcomings from the previous chapter
What classification performance are we trying to beat? Look back at previous chapter to find performance target
 % Any other literature that supports this requirement
 
 Brief introduction to personalisation. Remind user of definitions of source and target, transfer learning etc


% Establish your research problem
% In an empirical research paper, try to lead into the problem on the basis of your discussion of the literature. Think in terms of these questions:
% What research gap is your work intended to fill?
% What limitations in previous work does it address?
% What contribution to knowledge does it make?

Transfer learning approaches focus on reducing the labelling requirements for training data.
What factors affect this performance?
Demonstration of effective methods for individual personalisation of LSTM base HAR model. Effective being computationally efficient, data efficient and accurate



% Research question
% Present your research question clearly and directly, with a minimum of discussion at this point. The rest of the paper will be taken up with discussing and investigating this question; here you just need to express it.

How can a LSTM base HAR model be effectively trained to provide high levels of performance for an individual?
Can training burden be reduced through the use of a pre-trained starting model

Focusing on the data of an individual how performant a model can be achieved
Can a large population of data be effectively used to improve the performance of this individual trained model. "Given a small amount of labelled target data, how should we combine it during training with the large amount of labelled source data to achieve the lowest target error at test time?"

What are the aims of this chapter?
- Develop methods to produce models that perform better for an individual 
- Identify requirements for implementing these methods (data, computation)
- Demonstrate performance improvements and limitations of the methods
Relate these issues to the research questions for the thesis in general


% Sometimes an overview of the chapter
Within this chapter work is presented on investigating methods for achieving improved performance through model personalisation focusing on the data and computation requirements for this.



%------------------------------------------
\section{Related Works}
\acrshort{ml} classifiers are constructed with the assumption that the data distribution between the general source data domain and the new novels subject target domain are equal\cite{Farahani2020}. In reality this is never the case, as such a form of personalisation or domain adaption is required to allow for accurate classification in the target domain.

%What are the issues that we've identified and how are they commonly solved in literature.
Personalising of \acrshort{ml} models is a common issue and has been addressed in many different ways across different disciplines\hl{ADD SOME CITATIONS HERE}. From literature two common personalisation schemes for HAR models are seen --- retraining of a generic model using labelled target data; tuning of a domain mapping function which remaps input data. Both of these techniques take advantage of data from other subjects to reduce labelled data requirements for the target subject\cite{Shor2020}.

% Split this literature into the two categories

Wang et al does x
% Some generic descriptions of related works in this field x and y did this. This is a common theme etc
deep transfer learning - knowledge transfer between different data sets with sensors in different places\cite{Wang2018a} "when there are several source domains available, it is difficult to select the right source domains for transfer. The right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning" "Unsupervised Source Selection algo- rithm for Activity Recognition (USSAR). USSAR is able to select the most similar K source domains from a list of available domains. Af- ter this, we propose an effective Transfer Neural Network to perform knowledge transfer for Activity Recognition"

Fu et al
% Domain adaptation - Create mapping between similar subjects
Joint probability domain adaptation and improved pseudo labels\cite{Fu2021}. Manually selected features. Domain adaptation through selection of a mapping function between source and target features. 93.3\% accuracy of target domain

Yoon et al
% Retrain general model
Personalised language modelling using RNN - no examples of transfer learning for HAR in RNN. "It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are pro- posed for achieving the result."\cite{Yoon2017}

Ferrari et al
% Training from similar users
Similarity based personalisation strategies. Comparison of physical characteristics of different subjects\cite{Ferrari2020}. Adaboost classifier. Train using data from a general population, with and without some of the target subject's data. Similarity between different subjects is used to weight the influence of a subjects data.

% Combination - retrain general model based on similar users
Cruciani et al presents work on personalising an activity recognition model built from the subset of a general population. The subset of subjects was selected by comparing the similarity of manually selected features for the target subject and general training population. Those with the closest matching gait are used to generate the base model. Further training is then performed on this model using a small amount of the target subject data. This approach achieved a ~5\% improvement in performance when compared to selecting a subset at random\cite{Cruciani2020}. The experiment was performed on the \acrshort{adl} Extrasensory dataset published by Vaizman et al\cite{Vaizman2017}.

Research gaps
- Transfer learning in HAR has only used manually selected features - not learnt (deep) features
- Limited exploration of transfer learning for RNN in HAR

%------------------------------------------
\section{Methods and Materials}

%----------------------
% Data collection
Extended the data set previously captured for a select subset of participants. The same methods as presented in Section \ref{sec:methods-data-collection} were used to collect this additional data.

Larger quantities of data collected for 3 subjects least 7-mins cumulative time of each activity class. 

%------------------
% Data augmentation (Combining left and right ankle data)
Mixed Left and Right ankle data - justification for doing this.

Figure \ref{fig:ch6_target_subject_gyro_trends} shows the mean signals from the shank mounted gyroscope in the saggital plane for each of the Target subjects. Data from the left ankle has been transformed in order that both ankles appear in the same axis.

Data is not normalised

Only stair descent for subject 03 shows meaningful differences between left and right ankles. Therefore it is acceptable to use data from both ankles.

\begin{figure}[p]
    \begin{tabular}{lccc}
        & \textbf{Subject 01} & \textbf{Subject 03} & \textbf{Subject 09} \vspace{0.2cm}\\

        \rotatebox{90}{\enspace\qquad \textbf{Walking}} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_walking.pdf}\end{subfigure} & \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_walking.pdf}\end{subfigure} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_walking.pdf}\end{subfigure} \\
        
        \rotatebox{90}{~\quad \textbf{\glsentrylong{ra}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_up.pdf} \\
        
        \rotatebox{90}{\quad \textbf{\glsentrylong{rd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_down.pdf} \\
        
        \rotatebox{90}{~\quad \textbf{\glsentrylong{sa}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_up.pdf} \\
        
        \rotatebox{90}{\quad \textbf{\glsentrylong{sd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_down.pdf} \\
    \end{tabular}
    \centering
    
    % \hspace*{1cm}\includegraphics[width=0.7\textwidth]{content/6-Amputee/Gait-Trends/Legend.pdf}
    
    \caption[Angular velocity of the shank in the Saggital Plane during different activities for the three target subject]{Angular velocity of the shank in the Saggital Plane during different activities for the three target subject. The solid line shows the mean angular velocity for all steps recorded for each activity. The filled area represents the standard deviation. 0\% gait cycle is taken as peak swing for simplicity of calculation. The red, green and yellow lines are for the left ankles of Subjects 01, 03 and 09 respectively. The blue, purple and grey lines show the right ankles of Subjects 01, 03 and 09 respectively.}
    \label{fig:ch6_target_subject_gyro_trends}
\end{figure}

%--------------------------------
% Machine learning methods
To perform machine learning on just a single participant a different data division method is required than in the previous work. Instead using participants as the division point. Transitions between activity was used as previously described in \ref{par:methods-per-episode-division}.

What kind of ML model are we going to use?
% TODO: Add in LSTM model being tested
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{example-image-duck}
    \caption{Illustration of LSTM model that is being tested}
    \label{fig:ch5_illustration_of_base_LSTM_model}
\end{figure}

How were general models produced?

How are we going to perform our personalisation/transfer learning
% Train with a mixture of both target and source data
% Re-train a pre-trained model
% Freezing trainable parameters on set layers to 

%---------------------------
% Performance metrics
How are we going to determine performance? Categorical accuracy, Confusion Matrices, F1-Scores
% What would better than the baseline look like. Less data, faster at training, better performance

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{example-image-duck}
    \caption{LSTM Setups for transfer learning}
    \label{fig:ch5_LSTM_model_transfer_learning}
\end{figure}


%-------------------------------------------------------------------------------
\section{Results and Analysis}
The following sections contains the results and analysis of personalisation experiments. This split up into two sections, the first, Section \ref{subsec:ch5-baseline-model-results} looks at determining a baseline for personalisation performance. The second, Section \ref{subsec:ch5-model-personalisation-results} looks at new methods for improving this performance using data from additional individuals.

%-------------------------------------------------------------------------------
\subsection{Baseline Model}
\label{subsec:ch5-baseline-model-results}
Want to determine a baseline performance that can be achieved using only one individuals data. If performance of new methods does not exceed this value there is no benefit in the methods. Feed in increasing amounts of training data into the training process and see what performance improvements can be seen.

% Training performance vs quantity of data for both subjects
X axis is windows of data per class. Each window is 128 samples long at 100Hz, with a skip value of 3 samples so from 30 seconds per class to 450 seconds per class. Test samples was fixed at 5000 samples for all evaluations - 151 seconds

Figure \ref{fig:ch5_bespoke_mode_classification} shows the classification performance for the three Target subjects for different amounts of target data windows and different sized LSTM networks. Full data tables are available in Appendix \ref{sec:appendix-a-model-performance-bespoke}.

\begin{figure}[p]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_1.pdf}
        \caption{Subject 01}
        \label{fig:ch5_6_unit_bespoke_model}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_3.pdf}
        \caption{Subject 03}
        \label{fig:ch5_16_unit_bespoke_model}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_9.pdf}
        \caption{Subject 09}
        \label{fig:ch5_32_unit_bespoke_model}
    \end{subfigure}
    \caption[TODO Caption]{\hl{TODO Caption}}
    \label{fig:ch5_bespoke_mode_classification}
\end{figure}

Maximum performance achieved was 84.4\% for Subject 01, 88.5\% for Subject 03, and 82.6\% for Subject 09.

Performance improves rapidly from 100 to 1500 data samples, there is then a gradual increase in performance for the remaining increases in training windows.

Performance appears to still be increasing when the maximum amount of data was tested. Indicating further data would still improve performance. Not realistic to expose the model to every possible environment that it will need to operate on.

Standard deviation reduces with increasing data windows indicating more consistent performance across the 5 test sets.

Reduction in performance at 3000 samples for Subject 03 is likely due to model exposure to a new environment of data. Performance recovers with increasing amounts of data. Subject 09 also experiences similar drops in performance.

Increasing the number of units in general improved classification performance. This levels off at 32 units. Only the 6 unit model appears to have insufficient learning capacity. 

Increasing the number of units also reduced the number of epochs required to train the models. 

%Why is the model not achieving better performance?
By looking at the confusion matrices it can be seen which classes the models are struggling to identify. Table \ref{tab:ch5-bespoke-mode-confusion-matrix_subject_09} shows confusion matrices for the three targets subjects when the model is presented with test data.

\begin{table}[htbp]
    \centering
    \caption[Confusion matrix of test data for a 32 unit trained with \hl{x} target data LSTM model]{\hl{TODO: Put in correct values}Confusion matrix of test data for a 32 unit trained with \hl{x} target data LSTM model. (\acrfull{ra}, \acrfull{rd}, \acrfull{sa}, \acrfull{sd})}
    \label{tab:ch5-bespoke-mode-confusion-matrix_subject_09}
    \begin{subtable}{\textwidth}
    \caption{Subject 01}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK          & 100\% & 0 & 0 & 0 & 0 & 0 \\
         & \glsentryshort{ra} & 0 & 98\% & 2\% & 0 & 0 & 0 \\
         & \glsentryshort{rd} & 0 & 0 & 100\% & 0 & 0 & 0 \\
         & \glsentryshort{sa} & 0 & 0 & 0 & 100\% & 0 & 0 \\
         & \glsentryshort{sd} & 0 & 0 & 0 & 0 & 100\% & 0 \\
         & STOP          & 0 & 0 & 0 & 0 & 0 & 100\% \\
         \noalign{\hrule height 1.5pt}
    \end{tabularx}
    \end{subtable}
    \ \\ \vspace{0.3cm}
    \begin{subtable}{\textwidth}
    \caption{Subject 03}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK          & 100\% & 0 & 0 & 0 & 0 & 0 \\
         & \glsentryshort{ra} & 0 & 98\% & 2\% & 0 & 0 & 0 \\
         & \glsentryshort{rd} & 0 & 0 & 100\% & 0 & 0 & 0 \\
         & \glsentryshort{sa} & 0 & 0 & 0 & 100\% & 0 & 0 \\
         & \glsentryshort{sd} & 0 & 0 & 0 & 0 & 100\% & 0 \\
         & STOP          & 0 & 0 & 0 & 0 & 0 & 100\% \\
         \noalign{\hrule height 1.5pt}
    \end{tabularx}
    \end{subtable}
    \ \\ \vspace{0.3cm}
    \begin{subtable}{\textwidth}
    \caption{Subject 09}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK          & 100\% & 0 & 0 & 0 & 0 & 0 \\
         & \glsentryshort{ra} & 0 & 98\% & 2\% & 0 & 0 & 0 \\
         & \glsentryshort{rd} & 0 & 0 & 100\% & 0 & 0 & 0 \\
         & \glsentryshort{sa} & 0 & 0 & 0 & 100\% & 0 & 0 \\
         & \glsentryshort{sd} & 0 & 0 & 0 & 0 & 100\% & 0 \\
         & STOP          & 0 & 0 & 0 & 0 & 0 & 100\% \\
         \noalign{\hrule height 1.5pt}
    \end{tabularx}
    \end{subtable}
\end{table}

Talk about f-scores - indicate poor performance for Walking, Ramp Ascent and Ramp Descent
% Present graph of Learning rate

% Analysis points points
% - Why did subject x perform worse
% - Would more data help?
% - What baseline does this set for our subsequent work

%-------------------------------------------------------------------------------
\subsection{Model Personalisation}
\label{subsec:ch5-model-personalisation-results}
% Introductory paragraph

% Aims of this experiment - determine if using additional data from other individuals can improve the performance of a bespoke model

% \subsection{Results and Analysis}
% % Introductory Paragraph

%-------------------------------------------------------------------------------
\subsubsection{Mixing Target and Source Data}
% Fully trained model
% Present performance on this model - Categorical accuracy of training data, learning rate (epochs vs categorical accuracy)
\input{content/5-Personalisation/mixed-target-source-data-tables}

%-------------------------------------------------------------------------------
\subsubsection{Updating a General HAR model}
% Present results of test subject before bespoke testing - confusion matrix, categorical accuracy
The average accuracy for the 5 models before retraining was.
Subject 01 $76.5\%\pm3.1$, Subject 03 $81.5\%\pm4.3$ and Subject 09 $65.8\%\pm2.7$


% Present results of test subject after bespoke testing - confusion matrix, categorical accuracy, epochs vs categorical accuracy

% Analyse performance of this methods - model has not learning capacity left - need to train the pre-trained model less

% Use a range of different stopping points for the model
%Which method is used - what is the predicted performance improvement?
% Repeat for different pre-trained models, based on methods presented in the methodology section
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_pre_trained_model_accuracy.pdf}
    \caption[Retraining a pre-trained LSTM model]{\hl{Retraining a pre-trained LSTM model}}
    \label{fig:ch5_pretrained_model}
\end{figure}


% Mixed training data
% something akin to k-fold for time varying data (General Data set -> Individual Data set -> General Data set -> Individual Data set)

% Freezing individual layers - Illustration of which layers are frozen
%--------------------------------------------------------
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{content/5-Personalisation/ch5_frozen_dense_layer_accuracy.pdf}
    \caption[Retraining a pre-trained LSTM model]{Retraining a pre-trained LSTM model - frozen dense layer}
    \label{fig:ch5_freezing_dense_layer}
\end{figure}


%-------------------------------------------------------------------------------
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{content/5-Personalisation/ch5_frozen_lstm_layer_accuracy.pdf}
    \caption[Retraining a pre-trained LSTM model]{Retraining a pre-trained LSTM model - frozen LSTM layer}
    \label{fig:ch5_freezing_LSTM_layer}
\end{figure}

%-------------------------------------------------------------------------------
\section{Discussion}
What were we trying to achieve and have we achieved it?

Any interesting observations?

Bias the data set towards the target data, using the general population to supplement the limited environments experienced by the subject. Similar to Balanced Batch Learning \cite{Cruciani2020}

Reduced transition data from data set, previous work indicated these were a large source in inaccuracy.

Interesting how performance degrades at different increasing amounts of data. Suspicion is that this is because of the introduction of new environments of data. Limited control on data collection demonstrates problems with real world data. Encountering a new environment or mislabelling of data can result in degraded performance. Would be interesting to investigate how introduction of new environments affect performance but a new/different dataset collected in a controlled manner would be required for this

Do any of the methods show promise?

What are the limitations of this study/methods?

Areas for further work/improvements?

%-------------------------------------------------------------------------------
\section{Conclusions}
What were the research aims?

Have we met the research aims/outcomes of this work?

What are we going to do next? Apply methods to amputee data and determine if they are applicable