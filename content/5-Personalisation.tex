\chapter{Model Personalisation}
\label{chp:personalisation}

The previous chapter investigated classification accuracy for a subject agnostic \acrshort{lstm} based \acrshort{lmr} model. The outcome of this was that accuracies of greater than 80\% were not possible for unseen novel subjects. Instead a bespoke/personalised \acrshort{lmr} model is necessary to achieve higher performance. Within this chapter methods for achieving this will be explored.

% Relevant background on the topic - For a paper describing original research, you’ll instead provide an overview of the most relevant research that has already been conducted. This is a sort of miniature literature review—a sketch of the current state of research into your topic, boiled down to a few sentences.
The collection of labels from every individual is burdensome therefore any system that can be used to reduce the labelled data requirements is advantageous. This can be achieved through the use of a larger global source training set pooled from many individuals in the assumption they will hold information relevant to the target individual.\cite{Fallahzadeh2017, Schneider2021} 

Following the convention of transfer learning the personalisation subjects will be refereed to as the target. All other subjects which form prior knowledge will be referred to as source.

% Research question
% Present your research question clearly and directly, with a minimum of discussion at this point. The rest of the paper will be taken up with discussing and investigating this question; here you just need to express it.
Within this chapter we will investigate whether a large population of source data can be effectively used to improve the performance of a target individual's \acrshort{lstm} based \acrshort{lmr} classifier.

% Sometimes an overview of the chapter
The Chapter is presented as follows. Firstly, in Section \ref{sec:personalisation-related-works}, related literature is presented. This is followed by the methods and materials used in the study in Section \ref{sec:personalistaion-methods}. The results of a baseline model trained from only target data are presented in Section \ref{sec:personalisation-baseline-model-results}, followed by the results and analysis for penalisation techniques in Section \ref{sec:model-personalisation-results}.  Finally discussion and conclusions are presented in Sections \ref{sec:personalisation-discussion} and \ref{sec:personalisation-conclusions} respectively.


%------------------------------------------
\section{Related Works}
\label{sec:personalisation-related-works}
% Establish your research problem
% In an empirical research paper, try to lead into the problem on the basis of your discussion of the literature. Think in terms of these questions:
% What research gap is your work intended to fill?
% What limitations in previous work does it address?
% What contribution to knowledge does it make?
% What are common ways personalistaion is achieved

\acrshort{ml} classifiers are constructed with the assumption that the data distribution between the general source data domain and the new novels subject target domain are equal\cite{Farahani2020}. In reality this is never the case, as such a form of personalisation or domain adaption is required to allow for accurate classification in the target domain.

%What are the issues that we've identified and how are they commonly solved in literature.
Personalising of \acrshort{ml} models is a common issue and has been addressed in many different ways across different disciplines\cite{Mairittha2021, Tomanek2021}. Schneider et al divide personalising methods into two groups, shaping and data grouping\cite{Schneider2021}. The survey of literature will be divided into these two groups. Both of these techniques take advantage of data from other subjects to reduce labelled data requirements for the target subject\cite{Shor2020}.

% Shaping/transfer learning Early shaping train with target then source, late train with source then target, sample weighting - weighting the training data -- seems like data grouping.
In shaping the behaviour of a network is shaped toward a individual. This is commonly done in two ways early and late. In early shaping an target data is first used to train a model followed by source data. The opposite is done in late shaping. The last method is refereed to as transfer learning.\cite{Schneider2021}

Transfer learning is the ability to extend what has been learnt in one context to another nonidentical but similar context\cite{Fallahzadeh2017}. Transfer learning is appealing, since it is potentially faster as it is not necessary to train the model from scratch for each target individual.

By updated a trained model using data from a different distribution, an the knowledge from an existing model can be transferred. Where the model remains the same transfer learning is generally achieved in two phases, first a generic global model is trained from source data. Then it is personalised for the individual by additional training using only the targets data. The influence of the individual is controllable by both the number of iterations and number of layers trained.\cite{Schneider2021, Mireshghallah2021}

% EXAMPLES OF SHAPING/TRANSFER LEARNING IN HAR
Yoon et al
% Retrain general model
Personalised language modelling using RNN - no examples of transfer learning for HAR in RNN. "It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are pro- posed for achieving the result."\cite{Yoon2017}

Wang et al does x
deep transfer learning - knowledge transfer between different data sets with sensors in different places\cite{Wang2018a} "when there are several source domains available, it is difficult to select the right source domains for transfer. The right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning" "Unsupervised Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to select the most similar K source domains from a list of available domains. Af- ter this, we propose an effective Transfer Neural Network to perform knowledge transfer for Activity Recognition"

Fu et al
% Domain adaptation - Create mapping between similar subjects
Joint probability domain adaptation and improved pseudo labels\cite{Fu2021}. Manually selected features. Domain adaptation through selection of a mapping function between source and target features. 93.3\% accuracy of target domain


%-----------------------------
% Data grouping
``With data grouping, we aim at enlarging the data of an individual DI by adding similar data. Then, instead of shaping with the small dataset DI, we utilize the enlarged dataset of the individual I. While each individual differs from each other, it can be expected that either entire datasets of individuals are similar or that for a sample there exist similar samples (originating potentially from different individuals).''\cite{Schneider2021}

%EXAMPLES OF DATA GROUPING IN HAR
Ferrari et al
% Training from similar users
Similarity based personalisation strategies. Comparison of physical characteristics of different subjects\cite{Ferrari2020}. Adaboost classifier. Train using data from a general population, with and without some of the target subject's data. Similarity between different subjects is used to weight the influence of a subjects data.

Nguyen et al "Each subject might exhibit user-specific signal patterns, yet a group of users may perform activities in simi- lar manners and share analogous patterns. Leveraging this intuition, we explore Frechet Inception Distance (FID) as a distribution matching- based metric to measure the similarity between users. From that, we propose the nearest-FID-neighbors and the FID-graph clustering tech- niques to develop user-specific models that are trained with data from the community the testing user likely belongs to." \cite{Nguyen2021}


%-----------------------------
% Combination - retrain general model based on similar users
Cruciani et al presents work on personalising an activity recognition model built from the subset of a general population. The subset of subjects was selected by comparing the similarity of manually selected features for the target subject and general training population. Those with the closest matching gait are used to generate the base model. Further training is then performed on this model using a small amount of the target subject data. This approach achieved a ~5\% improvement in performance when compared to selecting a subset at random\cite{Cruciani2020}. The experiment was performed on the \acrshort{adl} Extrasensory dataset published by Vaizman et al\cite{Vaizman2017}.


Research gaps
- Transfer learning in HAR has only used manually selected features - not learnt (deep) features
- Limited exploration of transfer learning for RNN in HAR


%------------------------------------------
\section{Methods and Materials}
\label{sec:personalistaion-methods}
Introduction to section

% Data collection
Extended the data set previously captured for a select subset of participants. The same methods as presented in Section \ref{sec:methods-data-collection} were used to collect this additional data.

Larger quantities of data collected for 3 subjects least 7-mins cumulative time of each activity class. 

%------------------
% Data augmentation (Combining left and right ankle data)
Mixed Left and Right ankle data - justification for doing this.

Figure \ref{fig:ch6_target_subject_gyro_trends} shows the mean signals from the shank mounted gyroscope in the saggital plane for each of the Target subjects. Data from the left ankle has been transformed in order that both ankles appear in the same axis.

Data is not normalised

Only stair descent for subject 03 shows meaningful differences between left and right ankles. Therefore it is acceptable to use data from both ankles.

\begin{figure}[p]
    \begin{tabular}{lccc}
        & \textbf{Subject 01} & \textbf{Subject 03} & \textbf{Subject 09} \vspace{0.2cm}\\

        \rotatebox{90}{\enspace\qquad \textbf{Walking}} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_walking.pdf}\end{subfigure} & \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_walking.pdf}\end{subfigure} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_walking.pdf}\end{subfigure} \\
        
        \rotatebox{90}{~\quad \textbf{\glsentrylong{ra}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_up.pdf} \\
        
        \rotatebox{90}{\quad \textbf{\glsentrylong{rd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_down.pdf} \\
        
        \rotatebox{90}{~\quad \textbf{\glsentrylong{sa}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_up.pdf} \\
        
        \rotatebox{90}{\quad \textbf{\glsentrylong{sd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_down.pdf} \\
    \end{tabular}
    \centering
    
    % \hspace*{1cm}\includegraphics[width=0.7\textwidth]{content/6-Amputee/Gait-Trends/Legend.pdf}
    
    \caption[Angular velocity of the shank in the Saggital Plane during different activities for the three target subject]{Angular velocity of the shank in the Saggital Plane during different activities for the three target subject. The solid line shows the mean angular velocity for all steps recorded for each activity. The filled area represents the standard deviation. 0\% gait cycle is taken as peak swing for simplicity of calculation. The red, green and yellow lines are for the left ankles of Subjects 01, 03 and 09 respectively. The blue, purple and grey lines show the right ankles of Subjects 01, 03 and 09 respectively.}
    \label{fig:ch6_target_subject_gyro_trends}
\end{figure}

%--------------------------------
% Machine learning methods
To perform machine learning on just a single participant a different data division method is required than in the previous work. Instead using participants as the division point. Transitions between activity was used as previously described in \ref{par:methods-per-episode-division}.

What kind of ML model are we going to use?
% TODO: Add in LSTM model being tested
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{example-image-duck}
    \caption{Illustration of LSTM model that is being tested}
    \label{fig:ch5_illustration_of_base_LSTM_model}
\end{figure}

How were general models produced?

How are we going to perform our personalisation/transfer learning
% Train with a mixture of both target and source data
% Re-train a pre-trained model
% Freezing trainable parameters on set layers to 

%---------------------------
% Performance metrics
How are we going to determine performance? Categorical accuracy, Confusion Matrices, F1-Scores
% What would better than the baseline look like. Less data, faster at training, better performance

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{example-image-duck}
    \caption{LSTM Setups for transfer learning}
    \label{fig:ch5_LSTM_model_transfer_learning}
\end{figure}


%-------------------------------------------------------------------------------
\section{Baseline Model Performance}
\label{sec:personalisation-baseline-model-results}
Want to determine a baseline performance that can be achieved using only one individuals data. If performance of new methods does not exceed this value there is no benefit in the methods. Feed in increasing amounts of training data into the training process and see what performance improvements can be seen.

% Training performance vs quantity of data for both subjects
X axis is windows of data per class. Each window is 128 samples long at 100Hz, with a skip value of 3 samples so from 30 seconds per class to 450 seconds per class. Test samples was fixed at 5000 samples for all evaluations - 151 seconds

Figure \ref{fig:ch5_bespoke_mode_classification} shows the classification performance for the three Target subjects for different amounts of target data windows and different sized LSTM networks. Full data tables are available in Appendix \ref{sec:appendix-a-model-performance-bespoke}.

\begin{figure}[p]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_1.pdf}
        \caption{Subject 01}
        \label{fig:ch5_6_unit_bespoke_model}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_3.pdf}
        \caption{Subject 03}
        \label{fig:ch5_16_unit_bespoke_model}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_9.pdf}
        \caption{Subject 09}
        \label{fig:ch5_32_unit_bespoke_model}
    \end{subfigure}
    \caption[TODO Caption]{\hl{TODO Caption}}
    \label{fig:ch5_bespoke_mode_classification}
\end{figure}

Maximum performance achieved was 84.4\% for Subject 01, 88.5\% for Subject 03, and 82.6\% for Subject 09.

Performance improves rapidly from 100 to 1500 data samples, there is then a gradual increase in performance for the remaining increases in training windows.

Performance appears to still be increasing when the maximum amount of data was tested. Indicating further data would still improve performance. Not realistic to expose the model to every possible environment that it will need to operate on.

Standard deviation reduces with increasing data windows indicating more consistent performance across the 5 test sets.

Reduction in performance at 3000 samples for Subject 03 is likely due to model exposure to a new environment of data. Performance recovers with increasing amounts of data. Subject 09 also experiences similar drops in performance.

Increasing the number of units in general improved classification performance. This levels off at 32 units. Only the 6 unit model appears to have insufficient learning capacity. 

Increasing the number of units also reduced the number of epochs required to train the models. 

%Why is the model not achieving better performance?
By looking at the confusion matrices it can be seen which classes the models are struggling to identify. Table \ref{tab:ch5-bespoke-mode-confusion-matrix_subject_09} shows confusion matrices for the three targets subjects when the model is presented with test data.

\begin{table}[p]
    \centering
    \caption[Confusion matrix of test data for a 32 unit trained with \hl{x} target data LSTM model]{\hl{TODO: Put in correct values}Confusion matrix of test data for a 32 unit trained with \hl{x} target data LSTM model. (\acrfull{ra}, \acrfull{rd}, \acrfull{sa}, \acrfull{sd})}
    \label{tab:ch5-bespoke-mode-confusion-matrix_subject_09}
    \begin{subtable}{\textwidth}
    \caption{Subject 01}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 100\% & 0 & 0 & 0 & 0 & 0 \\
         & \glsentryshort{ra} & 0 & 98\% & 2\% & 0 & 0 & 0 \\
         & \glsentryshort{rd} & 0 & 0 & 100\% & 0 & 0 & 0 \\
         & \glsentryshort{sa} & 0 & 0 & 0 & 100\% & 0 & 0 \\
         & \glsentryshort{sd} & 0 & 0 & 0 & 0 & 100\% & 0 \\
         & STOP               & 0 & 0 & 0 & 0 & 0 & 100\% \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{Subject 03}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 100\% & 0 & 0 & 0 & 0 & 0 \\
         & \glsentryshort{ra} & 0 & 98\% & 2\% & 0 & 0 & 0 \\
         & \glsentryshort{rd} & 0 & 0 & 100\% & 0 & 0 & 0 \\
         & \glsentryshort{sa} & 0 & 0 & 0 & 100\% & 0 & 0 \\
         & \glsentryshort{sd} & 0 & 0 & 0 & 0 & 100\% & 0 \\
         & STOP               & 0 & 0 & 0 & 0 & 0 & 100\% \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{Subject 09}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 100\% & 0 & 0 & 0 & 0 & 0 \\
         & \glsentryshort{ra} & 0 & 98\% & 2\% & 0 & 0 & 0 \\
         & \glsentryshort{rd} & 0 & 0 & 100\% & 0 & 0 & 0 \\
         & \glsentryshort{sa} & 0 & 0 & 0 & 100\% & 0 & 0 \\
         & \glsentryshort{sd} & 0 & 0 & 0 & 0 & 100\% & 0 \\
         & STOP               & 0 & 0 & 0 & 0 & 0 & 100\% \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
\end{table}

Talk about f-scores - indicate poor performance for Walking, Ramp Ascent and Ramp Descent
% Present graph of Learning rate

% Analysis points points
% - Why did subject x perform worse
% - Would more data help?
% - What baseline does this set for our subsequent work


%-------------------------------------------------------------------------------
\section{Model Personalisation}
\label{sec:model-personalisation-results}
% Introductory paragraph
Introductory paragraph
% Aims of this experiment - determine if using additional data from other individuals can improve the performance of a bespoke model

% \subsection{Results and Analysis}
% % Introductory Paragraph

%-------------------------------------------------------------------------------
\subsection{Data Grouping}
Deep learning doesn't lend itself well to establishing similarities between different subjects. Instead attempt to bias the training though varying the proportion of test and training data.

% Fully trained model
% Present performance on this model - Categorical accuracy of training data, learning rate (epochs vs categorical accuracy)
\input{content/5-Personalisation/mixed-target-source-data-tables}

%-------------------------------------------------------------------------------
\subsection{Transfer Learning}
% Present results of test subject before bespoke testing - confusion matrix, categorical accuracy
The average accuracy for the 5 models before retraining was.
Subject 01 $76.5\%\pm3.1$, Subject 03 $81.5\%\pm4.3$ and Subject 09 $65.8\%\pm2.7$


% Present results of test subject after bespoke testing - confusion matrix, categorical accuracy, epochs vs categorical accuracy

% Analyse performance of this methods - model has not learning capacity left - need to train the pre-trained model less

% Use a range of different stopping points for the model
%Which method is used - what is the predicted performance improvement?
% Repeat for different pre-trained models, based on methods presented in the methodology section
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_pre_trained_model_accuracy.pdf}
    \caption[Retraining a pre-trained LSTM model]{\hl{Retraining a pre-trained LSTM model}}
    \label{fig:ch5_pretrained_model}
\end{figure}


% Mixed training data
% something akin to k-fold for time varying data (General Data set -> Individual Data set -> General Data set -> Individual Data set)

% Freezing individual layers - Illustration of which layers are frozen
%--------------------------------------------------------
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{content/5-Personalisation/ch5_frozen_dense_layer_accuracy.pdf}
    \caption[Retraining a pre-trained LSTM model]{Retraining a pre-trained LSTM model - frozen dense layer}
    \label{fig:ch5_freezing_dense_layer}
\end{figure}


%-------------------------------------------------------------------------------
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{content/5-Personalisation/ch5_frozen_lstm_layer_accuracy.pdf}
    \caption[Retraining a pre-trained LSTM model]{Retraining a pre-trained LSTM model - frozen LSTM layer}
    \label{fig:ch5_freezing_LSTM_layer}
\end{figure}

%-------------------------------------------------------------------------------
\section{Discussion}
\label{sec:personalisation-discussion}
What were we trying to achieve and have we achieved it?

Any interesting observations?

Bias the data set towards the target data, using the general population to supplement the limited environments experienced by the subject. Similar to Balanced Batch Learning \cite{Cruciani2020}

Reduced transition data from data set, previous work indicated these were a large source in inaccuracy.

Interesting how performance degrades at different increasing amounts of data. Suspicion is that this is because of the introduction of new environments of data. Limited control on data collection demonstrates problems with real world data. Encountering a new environment or mislabelling of data can result in degraded performance. Would be interesting to investigate how introduction of new environments affect performance but a new/different dataset collected in a controlled manner would be required for this

Do any of the methods show promise?

Identify requirements for implementing these methods (data, computation)

What are the limitations of this study/methods?

Areas for further work/improvements?

%-------------------------------------------------------------------------------
\section{Conclusions}
\label{sec:personalisation-conclusions}
What were the research aims?

Have we met the research aims/outcomes of this work?

What are we going to do next? Apply methods to amputee data and determine if they are applicable