\chapter{Model Personalisation}
\label{chp:personalisation}

The previous chapter investigated classification accuracy for a subject agnostic \acrshort{lstm} based \acrshort{lmr} model. The outcome of this was that accuracies of greater than 80\% were not possible for unseen novel subjects. Instead a bespoke/personalised \acrshort{lmr} model is necessary to achieve higher performance. Within this chapter methods for achieving this will be explored.

% Relevant background on the topic - For a paper describing original research, you’ll instead provide an overview of the most relevant research that has already been conducted. This is a sort of miniature literature review—a sketch of the current state of research into your topic, boiled down to a few sentences.
The collection of labels from every individual is burdensome therefore any system that can be used to reduce the labelled data requirements is advantageous. This can be achieved through the use of a larger global source training set pooled from many individuals in the assumption they will hold information relevant to the target individual.\cite{Fallahzadeh2017, Schneider2021} 

Following the convention of transfer learning the personalisation subjects will be refereed to as the target. All other subjects which form prior knowledge will be referred to as source.

% Research question
% Present your research question clearly and directly, with a minimum of discussion at this point. The rest of the paper will be taken up with discussing and investigating this question; here you just need to express it.
Within this chapter we will investigate whether a large population of source data can be effectively used to improve the performance of a target individual's \acrshort{lstm} based \acrshort{lmr} classifier.

% Sometimes an overview of the chapter
The Chapter is presented as follows. Firstly, in Section \ref{sec:personalisation-related-works}, related literature is presented. This is followed by the methods and materials used in the study in Section \ref{sec:personalistaion-methods}. The results of a baseline model trained from only target data are presented in Section \ref{sec:personalisation-baseline-model-results}, followed by the results and analysis for penalisation techniques in Section \ref{sec:model-personalisation-results}.  Finally discussion and conclusions are presented in Sections \ref{sec:personalisation-discussion} and \ref{sec:personalisation-conclusions} respectively.


%------------------------------------------
\section{Related Works}
\label{sec:personalisation-related-works}
% Establish your research problem
% In an empirical research paper, try to lead into the problem on the basis of your discussion of the literature. Think in terms of these questions:
% What research gap is your work intended to fill?
% What limitations in previous work does it address?
% What contribution to knowledge does it make?
% What are common ways personalisation is achieved

\acrshort{ml} classifiers are constructed with the assumption that the data distribution between the general source data domain and the new novels subject target domain are equal\cite{Farahani2020}. In reality this is never the case, as such a form of personalisation or domain adaption is required to allow for accurate classification in the target domain.

%What are the issues that we've identified and how are they commonly solved in literature.
Personalising of \acrshort{ml} models is a common issue and has been addressed in many different ways across different disciplines\cite{Mairittha2021, Tomanek2021}. Schneider et al divide personalising methods into two groups, shaping and data grouping\cite{Schneider2021}. The survey of literature will be divided into these two groups. Both of these techniques take advantage of data from other subjects to reduce labelled data requirements for the target subject\cite{Shor2020}.

% Shaping/transfer learning Early shaping train with target then source, late train with source then target, sample weighting - weighting the training data -- seems like data grouping.
In shaping the behaviour of a network is shaped toward a individual. This is commonly done in two ways early and late. In early shaping an target data is first used to train a model followed by source data. The opposite is done in late shaping. The last method is refereed to as transfer learning.\cite{Schneider2021}

Transfer learning is the ability to extend what has been learnt in one context to another nonidentical but similar context\cite{Fallahzadeh2017}. Transfer learning is appealing, since it is potentially faster as it is not necessary to train the model from scratch for each target individual.

By updated a trained model using data from a different distribution, an the knowledge from an existing model can be transferred. Where the model remains the same transfer learning is generally achieved in two phases, first a generic global model is trained from source data. Then it is personalised for the individual by additional training using only the targets data. The influence of the individual is controllable by both the number of iterations and number of layers trained.\cite{Schneider2021, Mireshghallah2021}

% EXAMPLES OF SHAPING/TRANSFER LEARNING IN HAR
Yoon et al
% Retrain general model
Personalised language modelling using RNN - no examples of transfer learning for HAR in RNN. "It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are pro- posed for achieving the result."\cite{Yoon2017}

Wang et al does x
deep transfer learning - knowledge transfer between different data sets with sensors in different places\cite{Wang2018a} ``when there are several source domains available, it is difficult to select the right source domains for transfer. The right source domain means that it has the most similar properties with the target domain, thus their similarity is higher, which can facilitate transfer learning'' ``Unsupervised Source Selection algorithm for Activity Recognition (USSAR). USSAR is able to select the most similar K source domains from a list of available domains. Af- ter this, we propose an effective Transfer Neural Network to perform knowledge transfer for Activity Recognition''

Fu et al
% Domain adaptation - Create mapping between similar subjects
Joint probability domain adaptation and improved pseudo labels\cite{Fu2021}. Manually selected features. Domain adaptation through selection of a mapping function between source and target features. 93.3\% accuracy of target domain


%-----------------------------
% Data grouping
``With data grouping, we aim at enlarging the data of an individual DI by adding similar data. Then, instead of shaping with the small dataset DI, we utilize the enlarged dataset of the individual I. While each individual differs from each other, it can be expected that either entire datasets of individuals are similar or that for a sample there exist similar samples (originating potentially from different individuals).''\cite{Schneider2021}

%EXAMPLES OF DATA GROUPING IN HAR
Ferrari et al
% Training from similar users
Similarity based personalisation strategies. Comparison of physical characteristics of different subjects\cite{Ferrari2020}. Adaboost classifier. Train using data from a general population, with and without some of the target subject's data. Similarity between different subjects is used to weight the influence of a subjects data.

Nguyen et al ``Each subject might exhibit user-specific signal patterns, yet a group of users may perform activities in simi- lar manners and share analogous patterns. Leveraging this intuition, we explore Frechet Inception Distance (FID) as a distribution matching- based metric to measure the similarity between users. From that, we propose the nearest-FID-neighbors and the FID-graph clustering tech- niques to develop user-specific models that are trained with data from the community the testing user likely belongs to.'' \cite{Nguyen2021}


%-----------------------------
% Combination - retrain general model based on similar users
Cruciani et al presents work on personalising an activity recognition model built from the subset of a general population. The subset of subjects was selected by comparing the similarity of manually selected features for the target subject and general training population. Those with the closest matching gait are used to generate the base model. Further training is then performed on this model using a small amount of the target subject data. This approach achieved a ~5\% improvement in performance when compared to selecting a subset at random\cite{Cruciani2020}. The experiment was performed on the \acrshort{adl} Extrasensory dataset published by Vaizman et al\cite{Vaizman2017}.


Research gaps
- Transfer learning in HAR has only used manually selected features - not learnt (deep) features
- Limited exploration of transfer learning for RNN in HAR
- No knowledge of how much data is required


%------------------------------------------
\section{Methods and Materials}
\label{sec:personalistaion-methods}
%Introduction to section
From the literature review we want to test transfer learning and some form of data grouping on our data set. Also like to establish how much data is required to achieve good performance for the target subject. Therefore we need a large set of data for a small number of subjects.

% Data collection
\subsection{Gait Data}
For these experiments we are only going to investigate the performance of models using the shank mounted accelerometer and gyroscope data as these performed best in the previous chapter.

Extended the data set previously captured for a select subset of participants (Subjects 01, 03 and 09). For each subject additional data war collected so that each class contained at least seven minutes cumulative time of each activity class. Data was collected in the same manner as described in Section \ref{sec:methods-data-collection}. For the general population the previous data set excluding Subjects 01, 03 and 09 was used.

%------------------
% Data augmentation (Combining left and right ankle data)
To further increase the amount of data available the data from both the left and right ankle were used with the left ankle transformed to match the right ankle. Figure \ref{fig:personalistaion_target_subjects_gyro_trends} shows the mean signals from the shank mounted gyroscope in the saggital plane for each of the Target subjects. Data is not normalised in Figure \ref{fig:personalistaion_target_subjects_gyro_trends}

Only stair descent for subject 03 shows any significant differences between left and right ankles. Therefore it was considered acceptable to use data from both ankles to enhance the quantity of data available

\begin{figure}[p]
    \begin{tabular}{lccc}
        & \textbf{Subject 01} & \textbf{Subject 03} & \textbf{Subject 09} \vspace{0.2cm}\\
        \rotatebox{90}{\enspace\qquad \textbf{Walking}} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_walking.pdf}\end{subfigure} & \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_walking.pdf}\end{subfigure} &
        \begin{subfigure}[b]{0.275\textwidth}\includegraphics[width=\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_walking.pdf}\end{subfigure} \\
        \rotatebox{90}{~\quad \textbf{\glsentrylong{ra}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_up.pdf} \\
        \rotatebox{90}{\quad \textbf{\glsentrylong{rd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_ramp_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_ramp_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_ramp_down.pdf} \\
        \rotatebox{90}{~\quad \textbf{\glsentrylong{sa}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_up.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_up.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_up.pdf} \\
        \rotatebox{90}{\quad \textbf{\glsentrylong{sd}}} & 
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_01_activity_stair_down.pdf} & \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_03_activity_stair_down.pdf} &
        \includegraphics[width=0.275\linewidth]{content/5-Personalisation/Gyro_Trends_For_Targets/ch5_gait_trends_subject_09_activity_stair_down.pdf} \\
    \end{tabular}
    \centering
    % \hspace*{1cm}\includegraphics[width=0.7\textwidth]{content/6-Amputee/Gait-Trends/Legend.pdf}
    \caption[Angular velocity of the shank in the Saggital Plane during different activities for the three target subject]{Angular velocity of the shank in the Saggital Plane during different activities for the three target subject. The solid line shows the mean angular velocity for all steps recorded for each activity. The filled area represents the standard deviation. 0\% gait cycle is taken as peak swing for simplicity of calculation. The red, green and yellow lines are for the left ankles of Subjects 01, 03 and 09 respectively. The blue, purple and grey lines show the right ankles of Subjects 01, 03 and 09 respectively.}
    \label{fig:personalistaion_target_subjects_gyro_trends}
\end{figure}

% Data division
The data needed to be divided into three data sets test, training and validation. The test set needs to be consistent for all experiments. The training/validation sets need adjustable in size to investigate changing the amount of available training data. Data division was achieved using the methods described in Section \ref{par:methods-per-episode-division}. To allow for repetitions of the same experiment multiple data sets were generated by shuffling the activity episodes order. Data windows were generated of 128 samples long with a skip of 2 samples between them. Each of the three sets of data was generated from a unique episodes.

All test sets were 5000 windows of target data. Training/validation set were of varying lengths - stated number of windows is the sum of training and validation windows. The sum is formed of a 70\%/30\% train/validation split.


%--------------------------------
% Machine learning methods
\subsection{\glsentrylong{ml} Methods} 
To perform machine learning on just a single participant a different data division method is required than in the previous work. Instead using participants as the division point. Transitions between activity was used as previously described in \ref{par:methods-per-episode-division}.

What kind of \acrshort{ml} model are we going to use?
% TODO: Add in LSTM model being tested
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{content/5-Personalisation/ch5_lstm_architecture.pdf}
    \caption[Illustration of \glsentryshort{lstm} machine learning model architecture]{Illustration of \acrshort{lstm} machine learning model architecture}
    \label{fig:ch5_illustration_of_base_LSTM_model}
\end{figure}

How were general models produced?

How are we going to perform our personalisation/transfer learning
% Train with a mixture of both target and source data
% Re-train a pre-trained model
% Freezing trainable parameters on set layers to 

%---------------------------
% Performance metrics
How are we going to determine performance? Categorical accuracy, Confusion Matrices, F1-Scores
% What would better than the baseline look like. Less data, faster at training, better performance


%-------------------------------------------------------------------------------
\section{Baseline Model Performance}
\label{sec:personalisation-baseline-model-results}
Want to determine a baseline performance that can be achieved using only one individuals data, or an un-tuned general model. If performance of new methods does not exceed these values there is no benefit in the methods. Feed in increasing amounts of training data into the training process and see what performance improvements can be seen.

% Performance of the trained general models with no fine-tuning
For the general models the average accuracy for the 5 models was Subject 01 $76.5\%\pm3.1$, Subject 03 $81.5\%\pm4.3$ and Subject 09 $65.8\%\pm2.7$. This was tested by averaging the performance of the all of the test sets used classified by each of the five models.

% Training performance vs quantity of data for both subjects
X axis is windows of data per class. Each window is 128 samples long at 100Hz, with a skip value of 3 samples so from 30 seconds per class to 450 seconds per class. Test samples was fixed at 5000 samples for all evaluations - 151 seconds

Figure \ref{fig:ch5_bespoke_mode_classification} shows the classification performance for the three Target subjects for different amounts of target data windows and different sized LSTM networks. Full data tables are available in Appendix \ref{sec:appendix-a-model-performance-bespoke}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_1.pdf}
        \caption{Subject 01}
        \label{fig:ch5_6_unit_bespoke_model}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_3.pdf}
        \caption{Subject 03}
        \label{fig:ch5_16_unit_bespoke_model}
    \end{subfigure}
    \caption[Classification performance for different size \glsentryshort{lstm} networks when trained with increasing amount of only target subject data windows.]{Classification performance for different size \acrshort{lstm} networks when trained with increasing amount of only target subject data windows. The line represents the mean of models trained, the filled area represents the standard deviation $(n=10)$. Each line show the classification performance for a different number of \acrshort{lstm} units. The red dot is 6 units, blue plus 16, purple cross 32 and yellow asterisk 64.}
    \label{fig:ch5_bespoke_mode_classification}
\end{figure}
\begin{figure}[t]\ContinuedFloat
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/Bespoke_Target/ch5_bespoke_target_model_subject_9.pdf}
        \caption{Subject 09}
        \label{fig:ch5_32_unit_bespoke_model}
    \end{subfigure}
    \caption[]{Classification performance for different size \acrshort{lstm} networks when trained with increasing amount of only target subject data windows (Cont.).}
\end{figure}

Maximum performance achieved was 84.4\% for Subject 01, 88.5\% for Subject 03, and 82.6\% for Subject 09.

Performance improves rapidly from 100 to 1500 data samples, there is then a gradual increase in performance for the remaining increases in training windows.

Performance appears to still be increasing when the maximum amount of data was tested. Indicating further data would still improve performance. Not realistic to expose the model to every possible environment that it will need to operate on.

Standard deviation reduces with increasing data windows indicating more consistent performance across the 5 test sets.

Reduction in performance at 3000 samples for Subject 03 is likely due to model exposure to a new environment of data. Performance recovers with increasing amounts of data. Subject 09 also experiences similar drops in performance.

Increasing the number of units in general improved classification performance. This levels off at 32 units. Only the 6 unit model appears to have insufficient learning capacity. 

Increasing the number of units also reduced the number of epochs required to train the models. 

%Why is the model not achieving better performance?
By looking at the confusion matrices it can be seen which classes the models are struggling to identify. Table \ref{tab:ch5-bespoke-mode-confusion-matrix_subject_09} shows confusion matrices for the three targets subjects when the model is presented with test data.

% This is the average confusion matrix for 5 models for each subject. Different episode offsets for each model

% columns represent the prediction labels and the rows represent the real labels
\begin{table}[p]
    \centering
    \caption[Test data confusion matrix for a 32 unit LSTM model trained with 15000 target data window]{Test data confusion matrix for a 32 unit LSTM model trained with 15000 target data window. Each value represent the percentage of correct predictions. (\acrfull{ra}, \acrfull{rd}, \acrfull{sa}, \acrfull{sd})}
    \label{tab:ch5-bespoke-mode-confusion-matrix_subject_09}
    \begin{subtable}{\textwidth}
    \caption{Subject 01}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 71.3 & 20.6 & 18.1 & 0.4 & 1.9 & 2.7 \\
         & \glsentryshort{ra} & 25.9 & 72.4 & 0.0 & 0.0 & 0.0 & 0.0 \\
         & \glsentryshort{rd} & 1.2 & 0.0 & 80.0 & 0.2 & 6.2 & 0.0 \\
         & \glsentryshort{sa} & 0.9 & 5.6 & 0.2 & 92.9 & 5.2 & 0.2 \\
         & \glsentryshort{sd} & 0.8 & 1.2 & 1.7 & 4.5 & 86.7 & 1.7 \\
         & STOP               & 0.1 & 0.1 & 0.0 & 2.0 & 0.1 & 95.4 \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{Subject 03}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 50.3 & 3.3 & 4.2 & 0.2 & 4.1 & 3.1 \\
         & \glsentryshort{ra} & 32.3 & 52.1 & 0.3 & 0.0 & 0.0 & 0.0 \\
         & \glsentryshort{rd} & 16.5 & 43.5 & 92.1 & 0.1 & 0.5 & 0.0 \\
         & \glsentryshort{sa} & 0.5 & 0.8 & 0.2 & 99.2 & 1.5 & 1.0 \\
         & \glsentryshort{sd} & 0.2 & 0.2 & 3.1 & 0.5 & 94.0 & 0.1 \\
         & STOP               & 0.1 & 0.0 & 0.0 & 0.0 & 0.1 & 95.7 \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{Subject 09}
    \begin{tabularx}{\textwidth}{ccYYYYYY}
        \noalign{\hrule height 1.5pt}
         & & \multicolumn{6}{c}{\textbf{Predicted Classes}} \\
         \hline
         & & WALK & \glsentryshort{ra} & \glsentryshort{rd} & \glsentryshort{sa} & \glsentryshort{sd} & STOP \\
         \multirow{6}{*}{\rotatebox{90}{\textbf{True Classes}}} 
         & WALK               & 90.4 & 7.6 & 6.3 & 2.0 & 2.7 & 0.0 \\
         & \glsentryshort{ra} & 2.2 & 82.5 & 0.1 & 1.4 & 0.1 & 0.0 \\
         & \glsentryshort{rd} & 5.6 & 7.4 & 85.1 & 2.1 & 8.8 & 0.0 \\
         & \glsentryshort{sa} & 1.0 & 1.7 & 0.6 & 92.8 & 4.7 & 0.1 \\
         & \glsentryshort{sd} & 0.8 & 0.6 & 8.0 & 1.6 & 83.4 & 0.7 \\
         & STOP               & 0.1 & 0.2 & 0.0 & 0.2 & 0.4 & 99.2 \\
         \noalign{\hrule height 1.5pt} \\
    \end{tabularx}
    \end{subtable}
\end{table}

Talk about f-scores - indicate poor performance for Walking, Ramp Ascent and Ramp Descent
% Present graph of Learning rate

% Analysis points points
% - Why did subject x perform worse
% - Would more data help?
% - What baseline does this set for our subsequent work


%-------------------------------------------------------------------------------
\section{Model Personalisation}
\label{sec:model-personalisation-results}
% Introductory paragraph
Introductory paragraph
% Aims of this experiment - determine if using additional data from other individuals can improve the performance of a bespoke model

% \subsection{Results and Analysis}
% % Introductory Paragraph

%-------------------------------------------------------------------------------
\subsection{Data Grouping}
Deep learning doesn't lend itself well to establishing similarities between different subjects. Instead attempt to bias the training though varying the proportion of test and training data.

% Fully trained model
% Present performance on this model - Categorical accuracy of training data, learning rate (epochs vs categorical accuracy)
\input{content/5-Personalisation/mixed-target-source-data-tables}

No obvious trends

Except for low values of source data with high values of target data - adding source data improves classification performance. 

More source windows than target windows always results in a increase in classification accuracy

Classification performance increases with increasing source data before falling off. It's not obvious where the point of degradation occurs. There is a band where performance is highest

Long training times - large number of epochs

%-------------------------------------------------------------------------------
\subsection{Transfer Learning}
Figure \ref{fig:ch5_pretrained_model} shows the classification performance for the three methods of fine-tuning the general models with increasing amounts of target training windows.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_pre_trained_model_accuracy.pdf}
        \caption{Fine tuning all layers}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_frozen_lstm_layer_accuracy.pdf}
        \caption{Fine tuning only the dense layers}
    \end{subfigure}
    \caption[Results of fine tuning a generic 32 unit \glsentryshort{lstm} model using increasing amounts of target data]{Results of fine tuning a generic 32 unit \acrshort{lstm} model using increasing amounts of target data. The solid line represents the mean classification performance for each amount of training windows. The filled area represents the standard deviation $(n=25)$. Each of the 3 target subjects is represented individually. The red dot line is Subject 01, blue plus is subject 03 and yellow cross is subject 09.}
    \label{fig:ch5_pretrained_model}
\end{figure}
\begin{figure}[t]\ContinuedFloat
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{content/5-Personalisation/ch5_frozen_dense_layer_accuracy.pdf}
        \caption{Fine tuning only the \acrshort{lstm} layers}
    \end{subfigure}
    \caption[]{Results of fine tuning a generic 32 unit \acrshort{lstm} model using increasing amounts of target data (Cont.).}
\end{figure}

Other than 100-200 for subject 03 all values are better than the un-tuned performance. These two values saw a marginal reduction in performance.

Very large improvement in performance early on

For subjects 03 and 09 performance drops down to the near the baseline performance around 3000 target samples. This is still an improvement over the general model for Subject 09 but drops below the general model for Subject 03

Freezing LSTM layer improved performance of Subject 01 by >1\%. Near no improvement for Subjects 03, and 09.

Freezing the dense layer improved performance of Subject 03, no improvement for 09 and reduction in performance for 01. There's no obvious reasons for these changes.


%-------------------------------------------------------------------------------
\section{Discussion}
\label{sec:personalisation-discussion}
%What were we trying to achieve and have we achieved it?
Reduce data requirements for a new subject - yep
Reduce training requirements - using transfer learning yep

%Do any of the methods show promise?
All of the methods showed a substantial performance improvements over the general model

All methods improved performance over the baseline model for the same quantity of target training samples

%Any interesting observations?
% Baseline model
Interesting how performance degrades at different increasing amounts of data. Suspicion is that this is because of the introduction of new environments of data. Limited control on data collection demonstrates problems with real world data. Encountering a new environment or mislabelling of data can result in degraded performance. Would be interesting to investigate how introduction of new environments affect performance but a new/different dataset collected in a controlled manner would be required for this.

This performance was 

% Data grouping
Performance was always better than the baseline model, when more source data than target data was used.

Mixed source and target data achieved better than transfer learning for lower amounts of target data windows. No predictable pattern to better classifications. Cost of substantially higher training epochs with more data so much slower.

With more selective source subjects this would probably have worked better. Additional measures would be needed to evaluate similarity between subjects, these are not needed when using purely deep learning approach.

%Transfer learning
Performance was always better than the baseline.

Improved over the baseline - performance was more predictable although there was still inconsistencies due to the training data sets used. This was similar behaviour to the baseline model.

Not obvious if freezing layers helped likely just noise


% Which was the best model?
For subjects 01 and 09 transfer learning is a better approach for subject 03 it's less obvious.

%Identify requirements for implementing these methods (data, computation)
Biggest jumps in performance over the baseline and the general model were shown for lower target samples. The 

%What are the limitations of this study/methods?
Reduced transition data from data set, previous work indicated these were a large source in inaccuracy.

Lots of room for hyperparamater investigations - only loosely tuned


%Areas for further work/improvements?
Bias the data set towards the target data, using the general population to supplement the limited environments experienced by the subject. Similar to Balanced Batch Learning \cite{Cruciani2020}

Could combine both methods - transfer learning with data grouping?
%-------------------------------------------------------------------------------
\section{Conclusions}
\label{sec:personalisation-conclusions}
What were the research aims?

Have we met the research aims/outcomes of this work?

What are we going to do next? Apply methods to amputee data and determine if they are applicable